From 121033e35f31cc22110e7618fb00ed84731937e4 Mon Sep 17 00:00:00 2001
From: LABBE Corentin <clabbe.montjoie@gmail.com>
Date: Thu, 22 Oct 2015 17:03:18 +0200
Subject: [PATCH] crypto: sun8i-ce Add Allwinner Crypto Engine cryptographic
 accelerator

Add support for the Crypto Engine included in Allwinner SoC H3 and A64.
The Crypto Engine is a hardware cryptographic accelerator that support:
- MD5 and SHA1/SHA224/SHA25/SHA384/SHA512 hash algorithms
- AES block cipher in CBC/ECB mode with 128/196/256bits keys.

Signed-off-by: Corentin Labbe <clabbe.montjoie@gmail.com>
---
 drivers/staging/sun8i-ss/Kconfig           |  12 +
 drivers/staging/sun8i-ss/Makefile          |   2 +
 drivers/staging/sun8i-ss/sun8i-ce-cipher.c | 392 ++++++++++++
 drivers/staging/sun8i-ss/sun8i-ce-core.c   | 669 +++++++++++++++++++++
 drivers/staging/sun8i-ss/sun8i-ce-hash.c   | 929 +++++++++++++++++++++++++++++
 drivers/staging/sun8i-ss/sun8i-ss.h        | 176 ++++++
 6 files changed, 2180 insertions(+)
 create mode 100644 drivers/staging/sun8i-ss/Kconfig
 create mode 100644 drivers/staging/sun8i-ss/Makefile
 create mode 100644 drivers/staging/sun8i-ss/sun8i-ce-cipher.c
 create mode 100644 drivers/staging/sun8i-ss/sun8i-ce-core.c
 create mode 100644 drivers/staging/sun8i-ss/sun8i-ce-hash.c
 create mode 100644 drivers/staging/sun8i-ss/sun8i-ss.h

diff --git a/drivers/staging/sun8i-ss/Kconfig b/drivers/staging/sun8i-ss/Kconfig
new file mode 100644
index 0000000..eeca62a
--- /dev/null
+++ b/drivers/staging/sun8i-ss/Kconfig
@@ -0,0 +1,12 @@
+config CRYPTO_DEV_SUN8I_SS
+	tristate "Support for Allwinner Crypto Engine cryptographic accelerator"
+	select CRYPTO_BLKCIPHER
+	select CRYPTO_ENGINE
+	help
+	  Select y here for having support for the crypto Engine availlable on
+	  Allwinner SoC H3 and A64.
+	  The Crypto Engine handle AES/DES/3DES ciphers in CBC mode
+	  and MD5, SHA1, SHA224, SHA256, SHA384 and SHA512 hash algorithms.
+
+	  To compile this driver as a module, choose M here: the module
+	  will be called sun8i-ss.
diff --git a/drivers/staging/sun8i-ss/Makefile b/drivers/staging/sun8i-ss/Makefile
new file mode 100644
index 0000000..c7ec027
--- /dev/null
+++ b/drivers/staging/sun8i-ss/Makefile
@@ -0,0 +1,2 @@
+obj-$(CONFIG_CRYPTO_DEV_SUN8I_SS) += sun8i-ss.o
+sun8i-ss-y += sun8i-ce-core.o sun8i-ce-hash.o sun8i-ce-cipher.o
diff --git a/drivers/staging/sun8i-ss/sun8i-ce-cipher.c b/drivers/staging/sun8i-ss/sun8i-ce-cipher.c
new file mode 100644
index 0000000..4a052d9
--- /dev/null
+++ b/drivers/staging/sun8i-ss/sun8i-ce-cipher.c
@@ -0,0 +1,392 @@
+/*
+ * sun8i-ce-cipher.c - hardware cryptographic accelerator for
+ * Allwinner H3/A64 SoC
+ *
+ * Copyright (C) 2016-2017 Corentin LABBE <clabbe.montjoie@gmail.com>
+ *
+ * This file add support for AES cipher with 128,192,256 bits keysize in
+ * CBC and ECB mode.
+ *
+ * You could find a link for the datasheet in Documentation/arm/sunxi/README
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+#include <linux/clk.h>
+#include <linux/irq.h>
+#include <linux/crypto.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/platform_device.h>
+#include <crypto/scatterwalk.h>
+#include <linux/scatterlist.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/reset.h>
+#include <crypto/sha.h>
+#include <crypto/internal/hash.h>
+#include <linux/dma-mapping.h>
+#include "sun8i-ss.h"
+
+int sun8i_ss_cipher(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun8i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun8i_ss_ctx *ss = op->ss;
+	struct sun8i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+	int flow = ss->flow;
+	struct ss_task *cet;
+	int nr_sgs, nr_sgd;
+	struct scatterlist *sg;
+	struct scatterlist *in_sg = areq->src;
+	struct scatterlist *out_sg = areq->dst;
+	int i;
+	u32 v;
+	int ret;
+	int no_chunk, chunked_src, chunked_dst;
+	int err = 0;
+	unsigned int todo, len;
+
+	/*dev_info(ss->dev, "%s %u %x\n", __func__, areq->nbytes, rctx->common);*/
+
+	chunked_src = 1;
+	sg = areq->src;
+	while (sg && chunked_src == 1) {
+		if ((sg->length % 4) != 0)
+			chunked_src = 0;
+		if (!IS_ALIGNED(sg->offset, sizeof(u32))) {
+			/*dev_info(ss->dev, "Align problem on src\n");*/
+			chunked_src = 0;
+		}
+		sg = sg_next(sg);
+	}
+	chunked_dst = 1;
+	sg = areq->dst;
+	while (sg && chunked_dst == 1) {
+		if ((sg->length % 4) != 0)
+			chunked_dst = 0;
+		if (!IS_ALIGNED(sg->offset, sizeof(u32))) {
+			/*dev_info(ss->dev, "Align problem on dst\n");*/
+			chunked_dst = 0;
+		}
+		sg = sg_next(sg);
+	}
+
+	if (chunked_src == 0 || chunked_dst == 0 || sg_nents(in_sg) > 8) {
+		struct blkcipher_desc fallback_desc = {
+			.tfm	= op->fallback_tfm,
+			.info	= areq->info,
+			.flags	= 0,
+		};
+		if (rctx->common & SS_DECRYPTION)
+			return crypto_blkcipher_decrypt_iv(&fallback_desc,
+				areq->dst, areq->src, areq->nbytes);
+		else
+			return crypto_blkcipher_encrypt_iv(&fallback_desc,
+				areq->dst, areq->src, areq->nbytes);
+	}
+
+	flow = rctx->flow;
+
+	mutex_lock(&ss->mutex);
+
+	cet = ss->tl[flow];
+	memset(cet, 0, sizeof(struct ss_task));
+
+	cet->t_id = flow;
+	cet->t_common_ctl = rctx->common | BIT(31);
+	cet->t_dlen = areq->nbytes / 4;
+
+	cet->t_sym_ctl = 0;
+	cet->t_sym_ctl |= rctx->sym;
+	cet->t_sym_ctl |= op->keymode;
+
+	for (i = 0; i < 8; i++) {
+		cet->t_src[i].len = 0;
+		cet->t_dst[i].len = 0;
+	}
+	cet->next = 0;
+
+	/*dev_info(ss->dev, "MAP key\n");*/
+	cet->t_key = dma_map_single(ss->dev, op->key, op->keylen,
+				    DMA_TO_DEVICE);
+	if (dma_mapping_error(ss->dev, cet->t_key)) {
+		dev_err(ss->dev, "Cannot DMA MAP KEY\n");
+		err = -EFAULT;
+		goto theend;
+	}
+
+	if (areq->info) {
+		if (!ss->chanlist[flow].bounce_iv)
+			ss->chanlist[flow].bounce_iv = kzalloc(512, GFP_KERNEL);
+		if (!ss->chanlist[flow].bounce_iv) {
+			err = -ENOMEM;
+			goto theend;
+		}
+		memcpy(ss->chanlist[flow].bounce_iv, areq->info,
+		       crypto_ablkcipher_ivsize(tfm));
+		/*dev_info(ss->dev, "MAP IV\n");*/
+		cet->t_iv = dma_map_single(ss->dev,
+					ss->chanlist[flow].bounce_iv,
+					crypto_ablkcipher_ivsize(tfm),
+					DMA_BIDIRECTIONAL);
+		if (dma_mapping_error(ss->dev, cet->t_iv)) {
+			dev_err(ss->dev, "Cannot DMA MAP IV\n");
+			err = -EFAULT;
+			goto theend;
+		}
+	}
+
+	/* check for chunked SGs */
+	no_chunk = 1;
+	sg = areq->src;
+	while (sg && no_chunk == 1) {
+		if ((sg->length % 4) != 0)
+			no_chunk = 0;
+		if (!IS_ALIGNED(sg->offset, sizeof(u32))) {
+			dev_info(ss->dev, "Align problem on src\n");
+			no_chunk = 0;
+		}
+		sg = sg_next(sg);
+	}
+	if (no_chunk == 0 || sg_nents(in_sg) > 8) {
+		dev_info(ss->dev, "Bounce src\n");
+		ret = sun8i_ss_bounce_src(areq, flow);
+		if (ret) {
+			dev_err(ss->dev, "Cannot bounce src\n");
+			err = -EFAULT;
+			goto theend;
+		}
+		in_sg = ss->chanlist[flow].bounce_src;
+	}
+	no_chunk = 1;
+	sg = areq->dst;
+	while (sg && no_chunk == 1) {
+		if ((sg->length % 4) != 0)
+			no_chunk = 0;
+		if (!IS_ALIGNED(sg->offset, sizeof(u32))) {
+			dev_info(ss->dev, "Align problem on dst\n");
+			no_chunk = 0;
+		}
+		sg = sg_next(sg);
+	}
+	if (no_chunk == 0) {
+		dev_info(ss->dev, "Bounce dst\n");
+		ret = sun8i_ss_bounce_dst(areq, flow);
+		if (ret) {
+			dev_err(ss->dev, "Cannot bounce dst\n");
+			err = -EFAULT;
+			goto theend;
+		}
+		out_sg = ss->chanlist[flow].bounce_dst;
+	}
+
+	if (in_sg == out_sg) {
+		nr_sgs = dma_map_sg(ss->dev, in_sg, sg_nents(in_sg),
+				    DMA_BIDIRECTIONAL);
+		if (nr_sgs < 0 || nr_sgs > 8) {
+			dev_info(ss->dev, "Invalid sg number %d\n", nr_sgs);
+			err = -EINVAL;
+			goto theend;
+		}
+		nr_sgd = nr_sgs;
+	} else {
+		nr_sgs = dma_map_sg(ss->dev, in_sg, sg_nents(in_sg),
+				    DMA_TO_DEVICE);
+		if (nr_sgs < 0 || nr_sgs > 8) {
+			dev_info(ss->dev, "Invalid sg number %d\n", nr_sgs);
+			err = -EINVAL;
+			goto theend;
+		}
+		nr_sgd = dma_map_sg(ss->dev, out_sg, sg_nents(out_sg),
+				    DMA_FROM_DEVICE);
+		if (nr_sgd < 0 || nr_sgd > 8) {
+			dev_info(ss->dev, "Invalid sg number %d\n", nr_sgd);
+			err = -EINVAL;
+			goto theend;
+		}
+	}
+
+	len = areq->nbytes;
+	for_each_sg(in_sg, sg, nr_sgs, i) {
+		cet->t_src[i].addr = sg_dma_address(sg);
+		todo = min(len, sg_dma_len(sg));
+		cet->t_src[i].len = todo / 4;
+		len -= todo;
+	}
+
+	len = areq->nbytes;
+	for_each_sg(out_sg, sg, nr_sgd, i) {
+		cet->t_dst[i].addr = sg_dma_address(sg);
+		todo = min(len, sg_dma_len(sg));
+		cet->t_dst[i].len = todo / 4;
+		len -= todo;
+	}
+
+	v = readl(ss->base + CE_ICR);
+	v |= 1 << flow;
+	writel(v, ss->base + CE_ICR);
+
+	reinit_completion(&ss->chanlist[flow].complete);
+	/* give the task address */
+	writel(ss->ce_t_phy[flow], ss->base + CE_TDQ);
+	ss->chanlist[flow].status = 0;
+
+	/* make sure that all configuration are written before enabling it */
+	wmb();
+
+	/* start the task */
+	writel(1, ss->base + CE_TLR);
+
+	wait_for_completion_interruptible_timeout(&ss->chanlist[flow].complete,
+						  msecs_to_jiffies(5000));
+	if (ss->chanlist[flow].status == 0) {
+		dev_err(ss->dev, "DMA timeout\n");
+		err = -EINVAL;
+	}
+
+	v = readl(ss->base + CE_ESR);
+	if (v)
+		dev_info(ss->dev, "CE ERROR %x\n", v);
+
+	/* disable interrupt */
+	v = readl(ss->base + CE_ICR);
+	v &= ~(1 << flow);
+	writel(v, ss->base + CE_ICR);
+
+	if (areq->info)
+		dma_unmap_single(ss->dev, cet->t_iv,
+				 crypto_ablkcipher_ivsize(tfm),
+				 DMA_BIDIRECTIONAL);
+
+	dma_unmap_single(ss->dev, cet->t_key, op->keylen, DMA_TO_DEVICE);
+	if (in_sg == out_sg) {
+		dma_unmap_sg(ss->dev, in_sg, nr_sgs, DMA_BIDIRECTIONAL);
+	} else {
+		dma_unmap_sg(ss->dev, in_sg, nr_sgs, DMA_TO_DEVICE);
+		dma_unmap_sg(ss->dev, out_sg, nr_sgd, DMA_FROM_DEVICE);
+	}
+
+	if (areq->dst != out_sg) {
+		dev_info(ss->dev, "Copy back\n");
+		sg_copy_from_buffer(areq->dst, sg_nents(areq->dst),
+				    ss->chanlist[flow].bufdst, areq->nbytes);
+		kfree(ss->chanlist[flow].bufsrc);
+		kfree(ss->chanlist[flow].bufdst);
+		ss->chanlist[flow].bufsrc = NULL;
+		ss->chanlist[flow].bufdst = NULL;
+		kfree(ss->chanlist[flow].bounce_src);
+		kfree(ss->chanlist[flow].bounce_dst);
+		ss->chanlist[flow].bounce_src = NULL;
+		ss->chanlist[flow].bounce_dst = NULL;
+	}
+theend:
+	mutex_unlock(&ss->mutex);
+
+	return err;
+}
+
+int sun8i_ss_cbc_aes_decrypt(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun8i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun8i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+	int e = get_engine_number(op->ss);
+
+	rctx->common = SS_OP_AES | SS_DECRYPTION;
+	rctx->sym = SS_CBC << 8;
+	rctx->flow = e;
+
+	return crypto_transfer_cipher_request_to_engine(op->ss->engines[e],
+							areq);
+}
+
+int sun8i_ss_cbc_aes_encrypt(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun8i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun8i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+	int e = get_engine_number(op->ss);
+
+	rctx->common = SS_OP_AES | SS_ENCRYPTION;
+	rctx->sym = SS_CBC << 8;
+	rctx->flow = e;
+
+	return crypto_transfer_cipher_request_to_engine(op->ss->engines[e],
+							areq);
+}
+
+int sun8i_ss_cipher_init(struct crypto_tfm *tfm)
+{
+	struct sun8i_tfm_ctx *op = crypto_tfm_ctx(tfm);
+	struct crypto_alg *alg = tfm->__crt_alg;
+	struct sun8i_ss_alg_template *algt;
+
+	memset(op, 0, sizeof(struct sun8i_tfm_ctx));
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.crypto);
+	op->ss = algt->ss;
+
+	tfm->crt_ablkcipher.reqsize = sizeof(struct sun8i_cipher_req_ctx);
+
+	op->fallback_tfm = crypto_alloc_blkcipher(crypto_tfm_alg_name(tfm),
+			0, CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK);
+	if (IS_ERR(op->fallback_tfm)) {
+		dev_err(op->ss->dev, "ERROR: Cannot allocate fallback\n");
+		return PTR_ERR(op->fallback_tfm);
+	}
+
+	return 0;
+}
+
+void sun8i_ss_cipher_exit(struct crypto_tfm *tfm)
+{
+	struct sun8i_tfm_ctx *op = crypto_tfm_ctx(tfm);
+
+	crypto_free_blkcipher(op->fallback_tfm);
+}
+
+int sun8i_ss_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+			unsigned int keylen)
+{
+	struct sun8i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun8i_ss_ctx *ss = op->ss;
+
+	switch (keylen) {
+	case 128 / 8:
+		op->keymode = SS_AES_128BITS;
+		break;
+	case 192 / 8:
+		op->keymode = SS_AES_192BITS;
+		break;
+	case 256 / 8:
+		op->keymode = SS_AES_256BITS;
+		break;
+	default:
+		dev_err(ss->dev, "ERROR: Invalid keylen %u\n", keylen);
+		crypto_ablkcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		return -EINVAL;
+	}
+	op->keylen = keylen;
+	op->key = kzalloc(keylen, GFP_KERNEL);
+	if (!op->key)
+		return -ENOMEM;
+	memcpy(op->key, key, keylen);
+
+	return crypto_blkcipher_setkey(op->fallback_tfm, key, keylen);
+}
+
+int handle_cipher_request(struct crypto_engine *engine,
+			  struct ablkcipher_request *breq)
+{
+	int err;
+
+	err = sun8i_ss_cipher(breq);
+	crypto_finalize_cipher_request(engine, breq, err);
+
+	return 0;
+}
+
diff --git a/drivers/staging/sun8i-ss/sun8i-ce-core.c b/drivers/staging/sun8i-ss/sun8i-ce-core.c
new file mode 100644
index 0000000..1a4defb
--- /dev/null
+++ b/drivers/staging/sun8i-ss/sun8i-ce-core.c
@@ -0,0 +1,669 @@
+/*
+ * sun8i-ce-core.c - hardware cryptographic accelerator for Allwinner H3/A64 SoC
+ *
+ * Copyright (C) 2015-2017 Corentin Labbe <clabbe.montjoie@gmail.com>
+ *
+ * Core file which registers crypto algorithms supported by the CryptoEngine.
+ *
+ * You could find a link for the datasheet in Documentation/arm/sunxi/README
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+#include <linux/clk.h>
+#include <linux/irq.h>
+#include <linux/crypto.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/platform_device.h>
+#include <crypto/scatterwalk.h>
+#include <linux/scatterlist.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/reset.h>
+#include <crypto/sha.h>
+#include <crypto/internal/hash.h>
+#include <linux/dma-mapping.h>
+
+#include "sun8i-ss.h"
+
+static const u32 ce_md5_init[MD5_DIGEST_SIZE / 4] = {
+	MD5_H0, MD5_H1, MD5_H2, MD5_H3
+};
+
+static const u32 ce_sha1_init[SHA1_DIGEST_SIZE / 4] = {
+	cpu_to_be32(SHA1_H0), cpu_to_be32(SHA1_H1),
+	cpu_to_be32(SHA1_H2), cpu_to_be32(SHA1_H3),
+	cpu_to_be32(SHA1_H4),
+};
+
+static const u32 ce_sha224_init[SHA256_DIGEST_SIZE / 4] = {
+	cpu_to_be32(SHA224_H0), cpu_to_be32(SHA224_H1),
+	cpu_to_be32(SHA224_H2), cpu_to_be32(SHA224_H3),
+	cpu_to_be32(SHA224_H4), cpu_to_be32(SHA224_H5),
+	cpu_to_be32(SHA224_H6), cpu_to_be32(SHA224_H7),
+};
+
+static const u32 ce_sha256_init[SHA256_DIGEST_SIZE / 4] = {
+	cpu_to_be32(SHA256_H0), cpu_to_be32(SHA256_H1),
+	cpu_to_be32(SHA256_H2), cpu_to_be32(SHA256_H3),
+	cpu_to_be32(SHA256_H4), cpu_to_be32(SHA256_H5),
+	cpu_to_be32(SHA256_H6), cpu_to_be32(SHA256_H7),
+};
+
+static const u64 ce_sha384_init[SHA512_DIGEST_SIZE / 8] = {
+	cpu_to_be64(SHA384_H0), cpu_to_be64(SHA384_H1),
+	cpu_to_be64(SHA384_H2), cpu_to_be64(SHA384_H3),
+	cpu_to_be64(SHA384_H4), cpu_to_be64(SHA384_H5),
+	cpu_to_be64(SHA384_H6), cpu_to_be64(SHA384_H7),
+};
+
+static const u64 ce_sha512_init[SHA512_DIGEST_SIZE / 8] = {
+	cpu_to_be64(SHA512_H0), cpu_to_be64(SHA512_H1),
+	cpu_to_be64(SHA512_H2), cpu_to_be64(SHA512_H3),
+	cpu_to_be64(SHA512_H4), cpu_to_be64(SHA512_H5),
+	cpu_to_be64(SHA512_H6), cpu_to_be64(SHA512_H7),
+};
+
+int get_engine_number(struct sun8i_ss_ctx *ss)
+{
+	int e = ss->flow;
+
+	ss->flow++;
+	if (ss->flow >= MAXCHAN)
+		ss->flow = 0;
+
+	return e;
+}
+
+/* compact an sglist to a more "compact" sglist
+ * With a maximum of 8 SGs
+ * */
+int sun8i_ss_compact(struct scatterlist *sg, unsigned int len)
+{
+	int numsg;
+	struct scatterlist *sglist;
+	int i;
+	void *buf;
+	unsigned int offset = 0;
+	int copied;
+
+	/* determine the number of sgs necessary */
+	numsg = len / PAGE_SIZE + 1;
+	if (numsg > 8)
+		return -EINVAL;
+	sglist = kcalloc(numsg, sizeof(struct scatterlist), GFP_KERNEL);
+	if (!sglist)
+		return -ENOMEM;
+	sg_init_table(sglist, numsg);
+	for (i = 0; i < numsg; i++) {
+		buf = kmalloc(PAGE_SIZE, GFP_KERNEL);
+		if (!buf)
+			return -ENOMEM;
+		sg_set_buf(&sglist[i], buf, PAGE_SIZE);
+		copied = sg_pcopy_to_buffer(sg, sg_nents(sg), buf, PAGE_SIZE, offset);
+		pr_info("%d Copied %d at %u\n", i, copied, offset);
+		offset += copied;
+	}
+	return 0;
+}
+
+/* copy all data from an sg to a plain buffer for channel flow */
+int sun8i_ss_bounce_src(struct ablkcipher_request *areq, int flow)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun8i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun8i_ss_ctx *ss = op->ss;
+
+	if (areq->nbytes > 4096)
+		return -EINVAL;
+
+	ss->chanlist[flow].bufsrc = kmalloc(areq->nbytes, GFP_KERNEL);
+	if (!ss->chanlist[flow].bufsrc)
+		return -ENOMEM;
+
+	sg_copy_to_buffer(areq->src, sg_nents(areq->src),
+			  ss->chanlist[flow].bufsrc, areq->nbytes);
+
+	ss->chanlist[flow].bounce_src = kcalloc(1, sizeof(struct scatterlist),
+						GFP_KERNEL);
+	if (!ss->chanlist[flow].bounce_src)
+		return -ENOMEM;
+
+	sg_init_table(ss->chanlist[flow].bounce_src, 1);
+	sg_set_buf(ss->chanlist[flow].bounce_src, ss->chanlist[flow].bufsrc,
+		   areq->nbytes);
+
+	return 0;
+}
+
+/* create a destination bounce buffer */
+int sun8i_ss_bounce_dst(struct ablkcipher_request *areq, int flow)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun8i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun8i_ss_ctx *ss = op->ss;
+
+	if (areq->nbytes > 4096)
+		return -EINVAL;
+
+	ss->chanlist[flow].bufdst = kmalloc(areq->nbytes, GFP_KERNEL);
+	if (!ss->chanlist[flow].bufdst)
+		return -ENOMEM;
+
+	ss->chanlist[flow].bounce_dst = kcalloc(1, sizeof(struct scatterlist),
+						GFP_KERNEL);
+	if (!ss->chanlist[flow].bounce_dst)
+		return -ENOMEM;
+
+	sg_init_table(ss->chanlist[flow].bounce_dst, 1);
+	sg_set_buf(ss->chanlist[flow].bounce_dst, ss->chanlist[flow].bufdst,
+		   areq->nbytes);
+
+	return 0;
+}
+/*
+int sun8i_ss_prep_src(struct ablkcipher_request *areq, int flow, struct scatterlist *in_sg)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun8i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun8i_ss_ctx *ss = op->ss;
+	unsigned int todo;
+	unsigned int len;
+	struct ss_task *cet;
+
+	cet = ss->tl[flow];
+	len = areq->nbytes;
+	for_each_sg(in_sg, sg, nr_sgs, i) {
+		cet->t_src[i].addr = sg_dma_address(sg);
+		todo = min(len, sg_dma_len(sg));
+		cet->t_src[i].len = todo / 4;
+		len -= todo;
+	}
+	return 0;
+}
+*/
+
+int handle_hash_request(struct crypto_engine *engine, struct ahash_request *areq)
+{
+	int err;
+	err = sun8i_hash(areq);
+	crypto_finalize_hash_request(engine, areq, err);
+
+	return 0;
+}
+
+irqreturn_t ss_irq_handler(int irq, void *data)
+{
+	u32 p;
+	struct sun8i_ss_ctx *ss = (struct sun8i_ss_ctx *)data;
+	int flow = 0;
+
+	p = readl(ss->base + CE_ISR);
+	/*dev_info(ss->dev, "%s %d, %x\n", __func__, irq, p);*/
+	for (flow = 0; flow < MAXCHAN; flow++) {
+		if (p & (1 << flow)) {
+			writel(1 << flow, ss->base + CE_ISR);
+			/*dev_info(ss->dev, "Acked %d\n", flow);*/
+			ss->chanlist[flow].status = 1;
+			complete(&ss->chanlist[flow].complete);
+		}
+	}
+
+	return IRQ_HANDLED;
+}
+
+static struct sun8i_ss_alg_template ss_algs[] = {
+{	.type = CRYPTO_ALG_TYPE_ABLKCIPHER,
+	.alg.crypto = {
+		.cra_name = "cbc(aes)",
+		.cra_driver_name = "cbc-aes-sun8i-ss",
+		.cra_priority = 300,
+		.cra_blocksize = AES_BLOCK_SIZE,
+		.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC |
+			CRYPTO_ALG_NEED_FALLBACK,
+		.cra_ctxsize = sizeof(struct sun8i_tfm_ctx),
+		.cra_module = THIS_MODULE,
+		.cra_alignmask = 3,
+		.cra_type = &crypto_ablkcipher_type,
+		.cra_init = sun8i_ss_cipher_init,
+		.cra_exit = sun8i_ss_cipher_exit,
+		.cra_ablkcipher = {
+			.min_keysize	= AES_MIN_KEY_SIZE,
+			.max_keysize	= AES_MAX_KEY_SIZE,
+			.ivsize		 = AES_BLOCK_SIZE,
+			.setkey		 = sun8i_ss_aes_setkey,
+			.encrypt		= sun8i_ss_cbc_aes_encrypt,
+			.decrypt		= sun8i_ss_cbc_aes_decrypt,
+		}
+	}
+},
+{	.type = CRYPTO_ALG_TYPE_AHASH,
+	.mode = SS_OP_MD5,
+	.hash_init = ce_md5_init,
+	.alg.hash = {
+		.init = sun8i_hash_init,
+		.update = sun8i_hash_update,
+		.final = sun8i_hash_final,
+		.finup = sun8i_hash_finup,
+		.digest = sun8i_hash_digest,
+		.export = sun8i_hash_export_md5,
+		.import = sun8i_hash_import_md5,
+		.halg = {
+			.digestsize = MD5_DIGEST_SIZE,
+			.statesize = sizeof(struct md5_state),
+			.base = {
+				.cra_name = "md5",
+				.cra_driver_name = "md5-sun8i-ss",
+				.cra_priority = 300,
+				.cra_alignmask = 3,
+				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
+					CRYPTO_ALG_ASYNC |
+					CRYPTO_ALG_NEED_FALLBACK,
+				.cra_blocksize = MD5_HMAC_BLOCK_SIZE,
+				.cra_ctxsize = sizeof(struct sun8i_hash_req_ctx),
+				.cra_module = THIS_MODULE,
+				.cra_type = &crypto_ahash_type,
+				.cra_init = sun8i_hash_crainit,
+			}
+		}
+	}
+},
+{	.type = CRYPTO_ALG_TYPE_AHASH,
+	.mode = SS_OP_SHA1,
+	.hash_init = ce_sha1_init,
+	.alg.hash = {
+		.init = sun8i_hash_init,
+		.update = sun8i_hash_update,
+		.final = sun8i_hash_final,
+		.finup = sun8i_hash_finup,
+		.digest = sun8i_hash_digest,
+		.export = sun8i_hash_export_sha1,
+		.import = sun8i_hash_import_sha1,
+		.halg = {
+			.digestsize = SHA1_DIGEST_SIZE,
+			.statesize = sizeof(struct sha1_state),
+			.base = {
+				.cra_name = "sha1",
+				.cra_driver_name = "sha1-sun8i-ss",
+				.cra_priority = 300,
+				.cra_alignmask = 3,
+				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
+					CRYPTO_ALG_ASYNC |
+					CRYPTO_ALG_NEED_FALLBACK,
+				.cra_blocksize = SHA1_BLOCK_SIZE,
+				.cra_ctxsize = sizeof(struct sun8i_hash_req_ctx),
+				.cra_module = THIS_MODULE,
+				.cra_type = &crypto_ahash_type,
+				.cra_init = sun8i_hash_crainit
+			}
+		}
+	}
+},
+{	.type = CRYPTO_ALG_TYPE_AHASH,
+	.mode = SS_OP_SHA224,
+	.hash_init = ce_sha224_init,
+	.alg.hash = {
+		.init = sun8i_hash_init,
+		.update = sun8i_hash_update,
+		.final = sun8i_hash_final,
+		.finup = sun8i_hash_finup,
+		.digest = sun8i_hash_digest,
+		.export = sun8i_hash_export_sha256,
+		.import = sun8i_hash_import_sha256,
+		.halg = {
+			.digestsize = SHA224_DIGEST_SIZE,
+			.statesize = sizeof(struct sha256_state),
+			.base = {
+				.cra_name = "sha224",
+				.cra_driver_name = "sha224-sun8i-ss",
+				.cra_priority = 300,
+				.cra_alignmask = 3,
+				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
+					CRYPTO_ALG_ASYNC |
+					CRYPTO_ALG_NEED_FALLBACK,
+				.cra_blocksize = SHA224_BLOCK_SIZE,
+				.cra_ctxsize = sizeof(struct sun8i_hash_req_ctx),
+				.cra_module = THIS_MODULE,
+				.cra_type = &crypto_ahash_type,
+				.cra_init = sun8i_hash_crainit
+			}
+		}
+	}
+},
+{	.type = CRYPTO_ALG_TYPE_AHASH,
+	.mode = SS_OP_SHA256,
+	.hash_init = ce_sha256_init,
+	.alg.hash = {
+		.init = sun8i_hash_init,
+		.update = sun8i_hash_update,
+		.final = sun8i_hash_final,
+		.finup = sun8i_hash_finup,
+		.digest = sun8i_hash_digest,
+		.export = sun8i_hash_export_sha256,
+		.import = sun8i_hash_import_sha256,
+		.halg = {
+			.digestsize = SHA256_DIGEST_SIZE,
+			.statesize = sizeof(struct sha256_state),
+			.base = {
+				.cra_name = "sha256",
+				.cra_driver_name = "sha256-sun8i-ss",
+				.cra_priority = 300,
+				.cra_alignmask = 3,
+				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
+					CRYPTO_ALG_ASYNC |
+					CRYPTO_ALG_NEED_FALLBACK,
+				.cra_blocksize = SHA256_BLOCK_SIZE,
+				.cra_ctxsize = sizeof(struct sun8i_hash_req_ctx),
+				.cra_module = THIS_MODULE,
+				.cra_type = &crypto_ahash_type,
+				.cra_init = sun8i_hash_crainit
+			}
+		}
+	}
+},
+{	.type = CRYPTO_ALG_TYPE_AHASH,
+	.mode = SS_OP_SHA384,
+	.hash_init = ce_sha384_init,
+	.alg.hash = {
+		.init = sun8i_hash_init,
+		.update = sun8i_hash_update,
+		.final = sun8i_hash_final,
+		.finup = sun8i_hash_finup,
+		.digest = sun8i_hash_digest,
+		.export = sun8i_hash_export_sha512,
+		.import = sun8i_hash_import_sha512,
+		.halg = {
+			.digestsize = SHA384_DIGEST_SIZE,
+			.statesize = sizeof(struct sha512_state),
+			.base = {
+				.cra_name = "sha384",
+				.cra_driver_name = "sha384-sun8i-ss",
+				.cra_priority = 300,
+				.cra_alignmask = 3,
+				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
+					CRYPTO_ALG_ASYNC |
+					CRYPTO_ALG_NEED_FALLBACK,
+				.cra_blocksize = SHA384_BLOCK_SIZE,
+				.cra_ctxsize = sizeof(struct sun8i_hash_req_ctx),
+				.cra_module = THIS_MODULE,
+				.cra_type = &crypto_ahash_type,
+				.cra_init = sun8i_hash_crainit
+			}
+		}
+	}
+},
+{	.type = CRYPTO_ALG_TYPE_AHASH,
+	.mode = SS_OP_SHA512,
+	.hash_init = ce_sha512_init,
+	.alg.hash = {
+		.init = sun8i_hash_init,
+		.update = sun8i_hash_update,
+		.final = sun8i_hash_final,
+		.finup = sun8i_hash_finup,
+		.digest = sun8i_hash_digest,
+		.export = sun8i_hash_export_sha512,
+		.import = sun8i_hash_import_sha512,
+		.halg = {
+			.digestsize = SHA512_DIGEST_SIZE,
+			.statesize = sizeof(struct sha512_state),
+			.base = {
+				.cra_name = "sha512",
+				.cra_driver_name = "sha512-sun8i-ss",
+				.cra_priority = 300,
+				.cra_alignmask = 3,
+				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
+					CRYPTO_ALG_ASYNC |
+					CRYPTO_ALG_NEED_FALLBACK,
+				.cra_blocksize = SHA512_BLOCK_SIZE,
+				.cra_ctxsize = sizeof(struct sun8i_hash_req_ctx),
+				.cra_module = THIS_MODULE,
+				.cra_type = &crypto_ahash_type,
+				.cra_init = sun8i_hash_crainit
+			}
+		}
+	}
+}
+
+};
+
+static int sun8i_ss_probe(struct platform_device *pdev)
+{
+	struct resource *res;
+	u32 v;
+	int err, i;
+	struct sun8i_ss_ctx *ss;
+
+	if (!pdev->dev.of_node)
+		return -ENODEV;
+
+	ss = devm_kzalloc(&pdev->dev, sizeof(*ss), GFP_KERNEL);
+	if (!ss)
+		return -ENOMEM;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	ss->base = devm_ioremap_resource(&pdev->dev, res);
+	if (IS_ERR(ss->base)) {
+		err = PTR_ERR(ss->base);
+		dev_err(&pdev->dev, "Cannot request MMIO %d\n", err);
+		return err;
+	}
+
+	ss->busclk = devm_clk_get(&pdev->dev, "ahb1_ce");
+	if (IS_ERR(ss->busclk)) {
+		err = PTR_ERR(ss->busclk);
+		dev_err(&pdev->dev, "Cannot get AHB SS clock err=%d\n", err);
+		return err;
+	}
+	dev_dbg(&pdev->dev, "clock ahb_ss acquired\n");
+
+	ss->ssclk = devm_clk_get(&pdev->dev, "mod");
+	if (IS_ERR(ss->ssclk)) {
+		err = PTR_ERR(ss->ssclk);
+		dev_err(&pdev->dev, "Cannot get SS clock err=%d\n", err);
+		return err;
+	}
+/*
+	err = clk_set_rate(ss->busclk, 300 * 1000 * 1000);
+	if (err)
+		dev_err(&pdev->dev, "Cannot set SS clk\n");*/
+
+	ss->reset = devm_reset_control_get_optional(&pdev->dev, "ahb");
+	if (IS_ERR(ss->reset)) {
+		if (PTR_ERR(ss->reset) == -EPROBE_DEFER)
+			return PTR_ERR(ss->reset);
+		dev_info(&pdev->dev, "no reset control found\n");
+		ss->reset = NULL;
+	}
+
+	/* Enable both clocks */
+	err = clk_prepare_enable(ss->busclk);
+	if (err != 0) {
+		dev_err(&pdev->dev, "Cannot prepare_enable busclk\n");
+		return err;
+	}
+
+	err = clk_prepare_enable(ss->ssclk);
+	if (err != 0) {
+		dev_err(&pdev->dev, "Cannot prepare_enable ssclk\n");
+		goto error_clk;
+	}
+	/* Deassert reset if we have a reset control */
+	if (ss->reset) {
+		err = reset_control_deassert(ss->reset);
+		if (err) {
+			dev_err(&pdev->dev, "Cannot deassert reset control\n");
+			goto error_ssclk;
+		}
+	}
+
+	ss->nsbase = ioremap(0x01c15800, 0x40);
+	if (ss->nsbase) {
+		v = readl(ss->nsbase + CE_CTR);
+		v &= 0x07;
+		dev_info(&pdev->dev, "CE_S Die ID %x\n", v);
+	}
+
+	v = readl(ss->base + CE_CTR);
+	v >>= 16;
+	v &= 0x07;
+	dev_info(&pdev->dev, "CE_NS Die ID %x\n", v);
+
+	ss->dev = &pdev->dev;
+	platform_set_drvdata(pdev, ss);
+
+	spin_lock_init(&ss->slock);
+
+	mutex_init(&ss->mutex);
+
+	for (i = 0; i < MAXCHAN; i++) {
+		init_completion(&ss->chanlist[i].complete);
+
+		ss->engines[i] = crypto_engine_alloc_init(ss->dev, 1);
+		if (!ss->engines[i]) {
+			dev_err(ss->dev, "Cannot request engine\n");
+			goto error_engine;
+		}
+		ss->engines[i]->cipher_one_request = handle_cipher_request;
+		ss->engines[i]->hash_one_request = handle_hash_request;
+		err = crypto_engine_start(ss->engines[i]);
+		if (err) {
+			dev_err(ss->dev, "Cannot request engine\n");
+			goto error_engine;
+		}
+	}
+	/* Get Secure IRQ */
+	ss->irq = platform_get_irq(pdev, 0);
+	if (ss->irq < 0) {
+		dev_err(ss->dev, "Cannot get S IRQ\n");
+		goto error_clk;
+	}
+
+	err = devm_request_irq(&pdev->dev, ss->irq, ss_irq_handler, 0,
+				   "sun8i-ce-s", ss);
+	if (err < 0) {
+		dev_err(ss->dev, "Cannot request S IRQ\n");
+		goto error_clk;
+	}
+
+	/* Get Non Secure IRQ */
+	ss->ns_irq = platform_get_irq(pdev, 1);
+	if (ss->ns_irq < 0) {
+		dev_err(ss->dev, "Cannot get NS IRQ\n");
+		goto error_clk;
+	}
+
+	err = devm_request_irq(&pdev->dev, ss->ns_irq, ss_irq_handler, 0,
+				   "sun8i-ce-ns", ss);
+	if (err < 0) {
+		dev_err(ss->dev, "Cannot request NS IRQ\n");
+		goto error_clk;
+	}
+
+	for (i = 0; i < MAXCHAN; i++) {
+		ss->tl[i] = dma_alloc_coherent(ss->dev, sizeof(struct ss_task),
+						   &ss->ce_t_phy[i], GFP_KERNEL);
+		if (!ss->tl[i]) {
+			dev_err(ss->dev, "Cannot get DMA memory for task %d\n",
+				i);
+			err = -EINVAL;
+			return err;
+		}
+	}
+
+	for (i = 0; i < ARRAY_SIZE(ss_algs); i++) {
+		ss_algs[i].ss = ss;
+		switch (ss_algs[i].type) {
+		case CRYPTO_ALG_TYPE_ABLKCIPHER:
+			err = crypto_register_alg(&ss_algs[i].alg.crypto);
+			if (err != 0) {
+				dev_err(ss->dev, "Fail to register %s\n",
+					ss_algs[i].alg.crypto.cra_name);
+				goto error_alg;
+			}
+			break;
+		case CRYPTO_ALG_TYPE_AHASH:
+			err = crypto_register_ahash(&ss_algs[i].alg.hash);
+			if (err != 0) {
+				dev_err(ss->dev, "Fail to register %s\n",
+					ss_algs[i].alg.hash.halg.base.cra_name);
+				goto error_alg;
+			}
+			break;
+		}
+	}
+
+	return 0;
+error_alg:
+	i--;
+	for (; i >= 0; i--) {
+		switch (ss_algs[i].type) {
+		case CRYPTO_ALG_TYPE_ABLKCIPHER:
+			crypto_unregister_alg(&ss_algs[i].alg.crypto);
+			break;
+		case CRYPTO_ALG_TYPE_AHASH:
+			crypto_unregister_ahash(&ss_algs[i].alg.hash);
+			break;
+		}
+	}
+	if (ss->reset)
+		reset_control_assert(ss->reset);
+error_engine:
+	while (i >= 0) {
+		crypto_engine_exit(ss->engines[i]);
+		i--;
+	}
+error_clk:
+	clk_disable_unprepare(ss->ssclk);
+error_ssclk:
+	clk_disable_unprepare(ss->busclk);
+	return err;
+}
+
+static int sun8i_ss_remove(struct platform_device *pdev)
+{
+	int i;
+	struct sun8i_ss_ctx *ss = platform_get_drvdata(pdev);
+
+	for (i = 0; i < ARRAY_SIZE(ss_algs); i++) {
+		switch (ss_algs[i].type) {
+		case CRYPTO_ALG_TYPE_ABLKCIPHER:
+			crypto_unregister_alg(&ss_algs[i].alg.crypto);
+			break;
+		case CRYPTO_ALG_TYPE_AHASH:
+			crypto_unregister_ahash(&ss_algs[i].alg.hash);
+			break;
+		}
+	}
+	for (i = 0; i < MAXCHAN; i++)
+		crypto_engine_exit(ss->engines[i]);
+
+	if (ss->reset)
+		reset_control_assert(ss->reset);
+	clk_disable_unprepare(ss->busclk);
+	return 0;
+}
+
+static const struct of_device_id h3_ss_crypto_of_match_table[] = {
+	{ .compatible = "allwinner,sun8i-h3-crypto" },
+	{}
+};
+MODULE_DEVICE_TABLE(of, h3_ss_crypto_of_match_table);
+
+static struct platform_driver sun8i_ss_driver = {
+	.probe		  = sun8i_ss_probe,
+	.remove		 = sun8i_ss_remove,
+	.driver		 = {
+		.name		   = "sun8i-ss",
+		.of_match_table	= h3_ss_crypto_of_match_table,
+	},
+};
+
+module_platform_driver(sun8i_ss_driver);
+
+MODULE_DESCRIPTION("Allwinner Security System cryptographic accelerator");
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Corentin Labbe <clabbe.montjoie@gmail.com>");
diff --git a/drivers/staging/sun8i-ss/sun8i-ce-hash.c b/drivers/staging/sun8i-ss/sun8i-ce-hash.c
new file mode 100644
index 0000000..abd25a7
--- /dev/null
+++ b/drivers/staging/sun8i-ss/sun8i-ce-hash.c
@@ -0,0 +1,929 @@
+/*
+ * sun8i-ce-hash.c - hardware cryptographic accelerator for Allwinner H3/A64 SoC
+ *
+ * Copyright (C) 2015-2017 Corentin Labbe <clabbe.montjoie@gmail.com>
+ *
+ * This file add support for MD5 and SHA1/SHA224/SHA256/SHA384/SHA512.
+ *
+ * You could find the datasheet in Documentation/arm/sunxi/README
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+#include "sun8i-ss.h"
+#include <linux/scatterlist.h>
+#include <crypto/internal/hash.h>
+#include <crypto/sha.h>
+#include <crypto/md5.h>
+
+/* This is a totally arbitrary value */
+#define SS_TIMEOUT 100
+#define SG_ZC 1
+
+/*#define DEBUG*/
+static int digest_size(struct ahash_request *areq)
+{
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sun8i_ss_alg_template *algt;
+	int digestsize;
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.hash);
+	digestsize = algt->alg.hash.halg.digestsize;
+	if (digestsize == SHA224_DIGEST_SIZE)
+		digestsize = SHA256_DIGEST_SIZE;
+	if (digestsize == SHA384_DIGEST_SIZE)
+		digestsize = SHA512_DIGEST_SIZE;
+	return digestsize;
+}
+
+int sun8i_hash_crainit(struct crypto_tfm *tfm)
+{
+	struct sun8i_tfm_ctx *op = crypto_tfm_ctx(tfm);
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->__crt_alg);
+	struct sun8i_ss_alg_template *algt;
+
+	memset(op, 0, sizeof(struct sun8i_tfm_ctx));
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.hash);
+	op->ss = algt->ss;
+
+	crypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),
+				 sizeof(struct sun8i_hash_req_ctx));
+
+#ifdef DEBUG
+	dev_info(op->ss->dev, "%s ====================\n", __func__);
+#endif
+	return 0;
+}
+
+int sun8i_hash_exit(struct ahash_request *areq)
+{
+/*	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);*/
+
+/*	crypto_free_shash(op->fallback_tfm);*/
+	return 0;
+}
+
+/* sun8i_hash_init: initialize request context */
+int sun8i_hash_init(struct ahash_request *areq)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sun8i_ss_alg_template *algt;
+
+	memset(op, 0, sizeof(struct sun8i_hash_req_ctx));
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.hash);
+	op->mode = algt->mode;
+
+	/* FALLBACK */
+	/*
+	op->fallback_tfm = crypto_alloc_shash(crypto_ahash_alg_name(tfm), 0,
+		CRYPTO_ALG_NEED_FALLBACK);
+	if (IS_ERR(op->fallback_tfm)) {
+		dev_err(algt->ss->dev, "Fallback driver cound no be loaded\n");
+		return PTR_ERR(op->fallback_tfm);
+	}*/
+
+	op->hash = kmalloc(digest_size(areq), GFP_KERNEL | GFP_ATOMIC);
+	if (!op->hash)
+		return -ENOMEM;
+	/*dev_info(algt->ss->dev, "Alloc %p\n", op->hash);*/
+	op->hash[0] = 0;
+#ifdef DEBUG
+	dev_info(algt->ss->dev, "%s ====================\n", __func__);
+#endif
+
+	return 0;
+}
+
+int sun8i_hash_export_md5(struct ahash_request *areq, void *out)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	struct md5_state *octx = out;
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sun8i_ss_alg_template *algt;
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.hash);
+
+	octx->byte_count = op->byte_count + op->blen;
+
+	if (op->blen > MD5_BLOCK_WORDS * 4) {
+		pr_err("Cannot export MD5\n");
+		return -EINVAL;
+	}
+
+	if (op->buf[0])
+		memcpy(octx->block, op->buf[0], op->blen);
+
+	if (op->byte_count > 0)
+		memcpy(octx->hash, op->hash, MD5_BLOCK_WORDS);
+	else
+		memcpy(octx->hash, algt->hash_init, MD5_BLOCK_WORDS);
+
+	return 0;
+}
+
+int sun8i_hash_import_md5(struct ahash_request *areq, const void *in)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	const struct md5_state *ictx = in;
+
+	sun8i_hash_init(areq);
+
+	op->byte_count = ictx->byte_count & ~0x3F;
+	op->blen = ictx->byte_count & 0x3F;
+
+	op->buf[0] = kzalloc(PAGE_SIZE, GFP_KERNEL | GFP_ATOMIC);
+	if (!op->buf[0])
+		return -ENOMEM;
+
+	if (!op->sgbounce[0])
+		op->sgbounce[0] = kzalloc(sizeof(*op->sgbounce[0]),
+					  GFP_KERNEL | GFP_ATOMIC);
+	if (!op->sgbounce[0])
+		return -ENOMEM;
+
+	sg_set_buf(op->sgbounce[0], op->buf[0], PAGE_SIZE);
+
+	if (op->blen)
+		memcpy(op->buf[0], ictx->block, op->blen);
+	memcpy(op->hash, ictx->hash, MD5_DIGEST_SIZE);
+
+	return 0;
+}
+
+int sun8i_hash_export_sha1(struct ahash_request *areq, void *out)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	struct sha1_state *octx = out;
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sun8i_ss_alg_template *algt;
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.hash);
+
+	if (op->blen > SHA1_DIGEST_SIZE * 4) {
+		pr_err("Cannot export SHA1\n");
+		return -EINVAL;
+	}
+
+	octx->count = op->byte_count + op->blen;
+
+	memcpy(octx->buffer, op->buf[0], op->blen);
+
+	if (op->byte_count > 0)
+		memcpy(octx->state, op->hash, SHA1_DIGEST_SIZE);
+	else
+		memcpy(octx->state, algt->hash_init, SHA1_DIGEST_SIZE);
+
+	return 0;
+}
+
+int sun8i_hash_import_sha1(struct ahash_request *areq, const void *in)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	const struct sha1_state *ictx = in;
+
+	sun8i_hash_init(areq);
+
+	op->byte_count = ictx->count & ~0x3F;
+	op->blen = ictx->count & 0x3F;
+
+	op->buf[0] = kzalloc(PAGE_SIZE, GFP_KERNEL);
+	if (!op->buf[0])
+		return -ENOMEM;
+
+	if (!op->sgbounce[0])
+		op->sgbounce[0] = kzalloc(sizeof(*op->sgbounce[0]), GFP_KERNEL);
+	if (!op->sgbounce[0])
+		return -ENOMEM;
+
+	sg_set_buf(op->sgbounce[0], op->buf[0], PAGE_SIZE);
+
+	if (op->blen)
+		memcpy(op->buf[0], ictx->buffer, op->blen);
+
+	memcpy(op->hash, ictx->state, SHA1_DIGEST_SIZE);
+
+	return 0;
+}
+
+int sun8i_hash_export_sha256(struct ahash_request *areq, void *out)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sun8i_ss_alg_template *algt;
+	struct sha256_state *octx = out;
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.hash);
+
+	if (op->blen > SHA224_DIGEST_SIZE * 4) {
+		pr_err("Cannot export SHA224\n");
+		return -EINVAL;
+	}
+
+	octx->count = op->byte_count + op->blen;
+	if (op->blen)
+		memcpy(octx->buf, op->buf[0], op->blen);
+
+	if (op->byte_count > 0)
+		memcpy(octx->state, op->hash, SHA256_DIGEST_SIZE);
+	else
+		memcpy(octx->state, algt->hash_init, SHA256_DIGEST_SIZE);
+
+	return 0;
+}
+
+int sun8i_hash_import_sha256(struct ahash_request *areq, const void *in)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	const struct sha256_state *ictx = in;
+
+	sun8i_hash_init(areq);
+
+	op->byte_count = ictx->count & ~0x3F;
+	op->blen = ictx->count & 0x3F;
+
+	op->buf[0] = kzalloc(PAGE_SIZE, GFP_KERNEL);
+	if (!op->buf[0])
+		return -ENOMEM;
+
+	if (!op->sgbounce[0])
+		op->sgbounce[0] = kzalloc(sizeof(*op->sgbounce[0]), GFP_KERNEL);
+	if (!op->sgbounce[0])
+		return -ENOMEM;
+
+	sg_set_buf(op->sgbounce[0], op->buf[0], PAGE_SIZE);
+
+	if (op->blen)
+		memcpy(op->buf[0], ictx->buf, op->blen);
+	memcpy(op->hash, ictx->state, SHA256_DIGEST_SIZE);
+
+	return 0;
+}
+
+int sun8i_hash_import_sha512(struct ahash_request *areq, const void *in)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	const struct sha512_state *ictx = in;
+
+	sun8i_hash_init(areq);
+
+	op->byte_count = ictx->count[0] & ~0x7F;
+	op->blen = ictx->count[0] & 0x7F;
+
+	op->byte_count2 = ictx->count[1];
+
+	op->buf[0] = kzalloc(PAGE_SIZE, GFP_KERNEL);
+	if (!op->buf[0])
+		return -ENOMEM;
+
+	if (!op->sgbounce[0])
+		op->sgbounce[0] = kzalloc(sizeof(*op->sgbounce[0]), GFP_KERNEL);
+	if (!op->sgbounce[0])
+		return -ENOMEM;
+
+	sg_set_buf(op->sgbounce[0], op->buf[0], PAGE_SIZE);
+
+	if (op->blen)
+		memcpy(op->buf[0], ictx->buf, op->blen);
+	memcpy(op->hash, ictx->state, SHA512_DIGEST_SIZE);
+
+	return 0;
+}
+
+int sun8i_hash_export_sha512(struct ahash_request *areq, void *out)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sun8i_ss_alg_template *algt;
+	struct sha512_state *octx = out;
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.hash);
+
+	if (op->blen > SHA512_DIGEST_SIZE * 4) {
+		pr_err("Cannot export SHA512\n");
+		return -EINVAL;
+	}
+
+	octx->count[1] = op->byte_count2;
+	octx->count[0] = op->byte_count + op->blen;
+	if (octx->count[0] < op->blen)
+		op->byte_count2++;
+
+	if (op->blen)
+		memcpy(octx->buf, op->buf[0], op->blen);
+
+	if (op->byte_count > 0)
+		memcpy(octx->state, op->hash, SHA512_DIGEST_SIZE);
+	else
+		memcpy(octx->state, algt->hash_init, SHA512_DIGEST_SIZE);
+	return 0;
+}
+
+int sun8i_ss_do_task(struct ahash_request *areq, int j)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sun8i_tfm_ctx *tfmctx = crypto_ahash_ctx(tfm);
+	struct sun8i_ss_ctx *ss = tfmctx->ss;
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sun8i_ss_alg_template *algt;
+	struct ss_task *cet;
+	int flow = 3;
+	int nr_sgb, i, todo, err = 0;
+	struct scatterlist *sg;
+	unsigned int len = j;
+	int digestsize;
+	u32 v;
+	int sgnum = 0;
+	int ret;
+
+	flow = op->flow;
+	/*dev_info(ss->dev, "Hash flow %d\n", flow);*/
+
+#ifdef DEBUG
+	dev_info(ss->dev, "%s %s %d\n", __func__, crypto_tfm_alg_name(areq->base.tfm), j);
+#endif
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.hash);
+	digestsize = algt->alg.hash.halg.digestsize;
+	if (digestsize == SHA224_DIGEST_SIZE)
+		digestsize = SHA256_DIGEST_SIZE;
+	if (digestsize == SHA384_DIGEST_SIZE)
+		digestsize = SHA512_DIGEST_SIZE;
+
+	cet = ss->tl[flow];
+	memset(cet, 0, sizeof(struct ss_task));
+	cet->t_id = flow;
+	cet->t_common_ctl = op->mode | BIT(31);
+
+	cet->t_dlen = j / 4;
+
+	nr_sgb = op->cursg + 1;
+	for (i = 0; i < nr_sgb; i++) {
+		sg = op->sgbounce[i];
+		if (!sg)
+			break;
+		/*dev_info(ss->dev, "SGmap %d\n", i);*/
+		ret = dma_map_sg(ss->dev, sg, 1, DMA_TO_DEVICE);
+		if (ret < 1)
+			dev_err(ss->dev, "SG DMA MAP ERROR\n");
+		cet->t_src[sgnum + i].addr = sg_dma_address(sg);
+		todo = min(len, sg_dma_len(sg));
+		cet->t_src[sgnum + i].len = todo / 4;
+#ifdef DEBUG
+		dev_info(ss->dev, "SG %d %u\n", sgnum + i, todo);
+#endif
+		len -= todo;
+	}
+
+	cet->t_dst[0].len = digestsize / 4;
+	cet->t_dst[0].addr = dma_map_single(ss->dev, op->hash, cet->t_dst[0].len, DMA_FROM_DEVICE);
+	if (dma_mapping_error(ss->dev, cet->t_dst[0].addr)) {
+		dev_err(ss->dev, "Cannot DMA MAP RESULT\n");
+	}
+
+	op->tiv = kmalloc(digestsize, GFP_KERNEL | GFP_ATOMIC);
+	if (!op->tiv)
+		return -ENOMEM;
+	if (op->hash[0] != 0) {
+		/*dev_info(ss->dev, "COPY IV %d\n", digestsize);*/
+		cet->t_common_ctl |= BIT(16);
+		memcpy(op->tiv, op->hash, digestsize);
+	}
+
+	cet->t_iv = dma_map_single(ss->dev, op->tiv, digestsize, DMA_TO_DEVICE);
+	if (dma_mapping_error(ss->dev, cet->t_iv)) {
+		dev_err(ss->dev, "Cannot DMA MAP IV\n");
+	}
+
+	/* LOCK */
+	v = readl(ss->base + CE_ICR);
+	v |= 1 << flow;
+	writel(v, ss->base + CE_ICR);
+
+	/*reinit_completion(&ss->chanlist[flow].complete);*/
+
+	writel(ss->ce_t_phy[flow], ss->base + CE_TDQ);
+	ss->chanlist[flow].status = 0;
+
+	/* TODO */
+	wmb();
+	writel(1, ss->base + CE_TLR);
+	/* UNLOCK */
+
+	/*while(ss->chanlist[flow].status == 0) {
+	}*/
+	wait_for_completion_interruptible_timeout(&ss->chanlist[flow].complete,
+						msecs_to_jiffies(5000));
+
+	if (ss->chanlist[flow].status == 0) {
+		dev_err(ss->dev, "DMA timeout\n");
+		err = -EINVAL;
+	}
+
+	dma_unmap_single(ss->dev, cet->t_iv, digestsize, DMA_TO_DEVICE);
+	for (i = 0; i < nr_sgb; i++) {
+		sg = op->sgbounce[i];
+		if (!sg)
+			break;
+		dma_unmap_sg(ss->dev, sg, 1, DMA_TO_DEVICE);
+	}
+
+	dma_unmap_single(ss->dev, cet->t_dst[0].addr, cet->t_dst[0].len,
+			 DMA_FROM_DEVICE);
+
+	v = readl(ss->base + CE_ESR);
+	if (v) {
+		dev_err(ss->dev, "CE ERROR %x %x\n", v, v >> flow * 4);
+	}
+	/*dev_info(ss->dev, "Fin Upload %d %u %p %p\n", op->blen, areq->nbytes,
+	 * op->buf, op->hash);*/
+	kfree(op->tiv);
+	op->tiv = NULL;
+	return 0;
+}
+
+#define SS_HASH_UPDATE 1
+#define SS_HASH_FINAL 2
+
+/* Fill op->sgbounce with sg from areq->src
+ * skip "skip" sg */
+int sun8i_ss_hashtask_zc(struct ahash_request *areq)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sun8i_tfm_ctx *tfmctx = crypto_ahash_ctx(tfm);
+	struct sun8i_ss_ctx *ss = tfmctx->ss;
+
+	int i = op->cursg;
+	int zlen = 0;
+
+	if (!op->src_sg) {
+		dev_err(ss->dev, "SG is NULL\n");
+		return -EINVAL;
+	}
+
+	do {
+		op->sgbounce[i] = op->src_sg;
+		op->sgflag[i] = SG_ZC;
+		zlen += op->src_sg->length;
+#ifdef DEBUG
+		dev_info(ss->dev, "ZCMap %d %u\n", i, op->src_sg->length);
+#endif
+		op->src_sg = sg_next(op->src_sg);
+		i++;
+		/* TODO round zlen to bs */
+		if (i > 7 && zlen > 64) {
+			op->cursg = i;
+			sun8i_ss_do_task(areq, zlen);
+
+			i = 0;
+			op->sgbounce[i] = NULL;
+			op->byte_count += zlen;
+			zlen = 0;
+			op->cursg = 0;
+		}
+		if (i > 7) {
+			dev_err(ss->dev, "%s Trop de SG\n", __func__);
+			return 0;
+		}
+	} while (op->src_sg);
+	op->cursg = i;
+
+#ifdef DEBUG
+	dev_info(ss->dev, "%s end with cursg=%d\n", __func__, i);
+#endif
+	return 0;
+}
+
+/* copy data in a compact sg
+ * copy data from areq->src offset ???
+ * to op->sgbounce on sg=op->cursg
+ */
+int sun8i_ss_hashtask_bounce(struct ahash_request *areq)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sun8i_tfm_ctx *tfmctx = crypto_ahash_ctx(tfm);
+	struct sun8i_ss_ctx *ss = tfmctx->ss;
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sun8i_ss_alg_template *algt;
+	int cursg = 0;
+	int offset;
+	int bs = 64;/* TODO */
+	unsigned long j;
+	unsigned long tocopy = 0;
+	int i;
+
+	int sg_src_offset = 0;
+	/* sg_src_len: how many bytes remains in SG src */
+	unsigned int sg_src_len = areq->nbytes;
+	unsigned long copied, max;
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.hash);
+	bs = algt->alg.hash.halg.base.cra_blocksize;
+
+start_copy:
+	/* in which sg we need to copy ? */
+	cursg = op->blen / PAGE_SIZE;
+	cursg = op->cursg;
+	offset = op->blen % PAGE_SIZE;
+
+	if (cursg > 7) {
+		dev_err(ss->dev, "ERROR: MAXIMUM SG\n");
+		return -EINVAL;
+	}
+
+/*	if (cursg == 0 && offset == 0 && !op->buf[0])
+		sg_init_table(op->sgbounce, 8);*/
+
+	if (!op->buf[cursg]) {
+		/*dev_info(ss->dev, "Allocate SG %d\n", cursg);*/
+		if (!op->sgbounce[cursg])
+			op->sgbounce[cursg] = kzalloc(sizeof(*op->sgbounce[cursg]),
+						      GFP_KERNEL | GFP_ATOMIC);
+		if (!op->sgbounce[cursg])
+			return -ENOMEM;
+
+		op->buf[cursg] = kzalloc(PAGE_SIZE, GFP_KERNEL | GFP_ATOMIC);
+		if (!op->buf[cursg])
+			return -ENOMEM;
+		sg_set_buf(op->sgbounce[cursg], op->buf[cursg], PAGE_SIZE);
+	}
+
+	if (areq->nbytes == 0)
+		return 0;
+
+	/* if the request is not final, we need to round to bs */
+	if ((op->flags & SS_HASH_FINAL) == 0 && sg_src_len + op->blen > bs)
+		max = ((sg_src_len + op->blen) / bs) * bs - op->blen;
+	else
+		max = sg_src_len;
+
+	tocopy = min(max, PAGE_SIZE - offset);
+
+	if (tocopy == 0) {
+#ifdef DEBUG
+		dev_info(ss->dev, "Nocopy %d (sg %d) (src offset %d/%u) %lu %lu\n",
+			 offset, cursg, sg_src_offset, areq->nbytes, max, tocopy);
+#endif
+		return 0;
+	}
+
+	copied = sg_pcopy_to_buffer(op->src_sg, sg_nents(op->src_sg),
+				    op->buf[cursg] + offset, tocopy,
+				    sg_src_offset);
+#ifdef DEBUG
+	dev_info(ss->dev, "Copied %lu at %d (sg %d) (src offset %d/%u) %lu %lu sgsrclen=%u\n",
+		 copied, offset, cursg, sg_src_offset, areq->nbytes, max, tocopy, sg_src_len);
+#endif
+	sg_src_len -= copied;
+	sg_src_offset += copied;
+	op->blen += copied;
+	if (op->blen % PAGE_SIZE == 0)
+		op->cursg++;
+
+	/* maximum supported by hw */
+	if (cursg == 7 && op->blen == PAGE_SIZE) {
+#ifdef DEBUG
+		dev_info(ss->dev, "NEED UPLOAD (MAXIMUM)\n");
+#endif
+	}
+
+	if (sg_src_len > bs)
+		goto start_copy;
+
+	if (op->blen >= bs && (op->flags & SS_HASH_FINAL) == 0) {
+		j = op->blen - op->blen % bs;
+#ifdef DEBUG
+		dev_info(ss->dev, "NEED UPLOAD %lu sg_src_len=%d\n", j, sg_src_len);
+#endif
+		/*sg_mark_end(op->sgbounce[cursg]);*/
+		sun8i_ss_do_task(areq, j);
+		op->cursg = 0;
+		op->byte_count += j;
+		memset(op->buf[0], 0, PAGE_SIZE);
+		/*sg_init_table(op->sgbounce, 8);*/
+		for (i = 0; i < 8; i++) {
+			if (op->buf[i]) {
+				memset(op->sgbounce[i], 0, sizeof(struct scatterlist));
+				sg_set_buf(op->sgbounce[i], op->buf[i], PAGE_SIZE);
+			}
+		}
+		op->blen = 0;
+	}
+
+	if (sg_src_len > 0)
+		goto start_copy;
+
+	op->cursg = cursg;
+
+#ifdef DEBUG
+	dev_info(ss->dev, "%s end %llu %lu\n", __func__, op->byte_count, op->blen);
+#endif
+	return 0;
+}
+
+/*
+ * sun8i_hash_update: update hash engine
+ *
+ * Could be used for both SHA1 and MD5
+ * Write data by step of 32bits and put then in the SS.
+ *
+ * Since we cannot leave partial data and hash state in the engine,
+ * we need to get the hash state at the end of this function.
+ * We can get the hash state every 64 bytes
+ *
+ * So the first work is to get the number of bytes to write to SS modulo 64
+ * The extra bytes will go to a temporary buffer op->buf storing op->blen bytes
+ *
+ * So at the begin of update()
+ * if op->blen + areq->nbytes < 64
+ * => all data will be written to wait buffer (op->buf) and end=0
+ * if not, write all data from op->buf to the device and position end to
+ * complete to 64bytes
+ *
+ * example 1:
+ * update1 60o => op->blen=60
+ * update2 60o => need one more word to have 64 bytes
+ * end=4
+ * so write all data from op->buf and one word of SGs
+ * write remaining data in op->buf
+ * final state op->blen=56
+ */
+int sun8i_hash(struct ahash_request *areq)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sun8i_tfm_ctx *tfmctx = crypto_ahash_ctx(tfm);
+	struct sun8i_ss_ctx *ss = tfmctx->ss;
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sun8i_ss_alg_template *algt;
+	int err = 0;
+	int no_chunk = 1;
+	struct scatterlist *sg;
+	unsigned int index, padlen;
+	int j;
+	int zeros;
+	__be64 bits;
+	int digestsize;
+	int bs = 64;
+	int cursg;
+	int i;
+	int rawdata = 0;
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.hash);
+	digestsize = algt->alg.hash.halg.digestsize;
+	if (digestsize == SHA224_DIGEST_SIZE)
+		digestsize = SHA256_DIGEST_SIZE;
+	if (digestsize == SHA384_DIGEST_SIZE)
+		digestsize = SHA512_DIGEST_SIZE;
+	bs = algt->alg.hash.halg.base.cra_blocksize;
+
+	if (!op->src_sg)
+		op->src_sg = areq->src;
+
+	i = sg_nents(op->src_sg);
+	if (i > 7) {
+		dev_err(ss->dev, "MAXIMUM SG %d\n", i);
+		return -EINVAL;
+	}
+
+#ifdef DEBUG
+	dev_info(ss->dev, "%s nbytes=%u flags=%x blen=%lu %s\n", __func__, areq->nbytes,
+		 op->flags, op->blen, crypto_tfm_alg_name(areq->base.tfm));
+#endif
+
+	if ((op->flags & SS_HASH_UPDATE) == 0)
+		goto hash_final2;
+
+	/* If we cannot work on a full block and more data could come, bounce data */
+	if (op->blen + areq->nbytes < bs && (op->flags & SS_HASH_FINAL) == 0)
+		return sun8i_ss_hashtask_bounce(areq);
+
+	/* does the data need to be bounced ? */
+	sg = op->src_sg;
+	while (sg && no_chunk == 1) {
+		if ((sg->length % 4) != 0)
+			no_chunk = 0;
+		if (!IS_ALIGNED(sg->offset, sizeof(u32))) {
+#ifdef DEBUG
+			dev_info(ss->dev, "Align problem on src\n");
+#endif
+			no_chunk = 0;
+		}
+		sg = sg_next(sg);
+	}
+
+	if (no_chunk == 0 || areq->nbytes == 0 || (areq->nbytes % bs != 0) || op->blen > 0) {
+		sun8i_ss_hashtask_bounce(areq);
+	} else {
+		sun8i_ss_hashtask_zc(areq);
+		if (areq->nbytes >= bs && (op->flags & SS_HASH_FINAL) == 0) {
+			op->cursg--;
+			j = areq->nbytes - areq->nbytes % bs;
+#ifdef DEBUG
+			dev_info(ss->dev, "Will upload %d of %u\n", j, areq->nbytes);
+#endif
+			sun8i_ss_do_task(areq, j);
+			if (op->buf[0])
+				memset(op->buf[0], 0, PAGE_SIZE);
+			op->cursg = 0;
+			op->byte_count += j;
+			op->blen = 0;
+			for (i = 0; i < 8; i++) {
+				/*dev_info(ss->dev, "Clean %d f=%d %p %p\n", i, op->sgflag[i], op->buf[i], op->sgbounce[i]);*/
+				/* If sg is a bounce sg, zero it*/
+				if (op->buf[i] && op->sgflag[i] == 0) {
+					if (op->sgbounce[i]) {
+						memset(op->sgbounce[i], 0, sizeof(struct scatterlist));
+						sg_set_buf(op->sgbounce[i],
+							   op->buf[i], PAGE_SIZE);
+					}
+				} else {
+					op->sgflag[i] = 0;
+				}
+			}
+			return 0;
+		}
+		/*op->blen = areq->nbytes;*/
+		rawdata = areq->nbytes; /*TODO a retablir avec le zc ?*/
+	}
+/*
+	if (op->blen >= bs && (op->flags & SS_HASH_FINAL) == 0) {
+		j = op->blen - op->blen % bs;
+		dev_info(ss->dev, "Will upload %d of %lu\n", j, op->blen);
+	}*/
+
+	if ((op->flags & SS_HASH_FINAL) == 0)
+		return 0;
+
+hash_final2:
+
+	cursg = op->blen / PAGE_SIZE;
+	cursg = op->cursg;
+	if (!op->buf[cursg]) {
+		/*dev_err(ss->dev, "No buffer on sg %d\n", cursg);*/
+		op->buf[cursg] = kzalloc(PAGE_SIZE, GFP_KERNEL | GFP_ATOMIC);
+		if (!op->buf[cursg])
+			return -ENOMEM;
+		if (!op->sgbounce[cursg])
+			op->sgbounce[cursg] = kzalloc(sizeof(*op->sgbounce[cursg]),
+						      GFP_KERNEL | GFP_ATOMIC);
+		if (!op->sgbounce[cursg])
+			return -ENOMEM;
+		sg_set_buf(op->sgbounce[cursg], op->buf[cursg], PAGE_SIZE);
+		/*dev_info(ss->dev, "Alloc SG %d %lu\n", cursg, PAGE_SIZE);*/
+		/*return -EINVAL;*/
+	}
+
+	j = op->blen;
+	op->byte_count += (op->blen / 4) * 4 + rawdata;
+	op->buf[cursg][j] = 1 << 7;
+	j += 4;
+
+	if (op->mode == SS_OP_MD5 || op->mode == SS_OP_SHA1 ||
+	    op->mode == SS_OP_SHA224 || op->mode == SS_OP_SHA256) {
+		index = (op->byte_count + 4) & 0x3f;
+		op->byte_count += op->blen % 4;
+		padlen = (index < 56) ? (56 - index) : (120 - index);
+		zeros = padlen;
+	} else {
+		op->byte_count += op->blen % 4;
+		index = (op->byte_count + 4) & 0x7f;
+		padlen = (index < 112) ? (112 - index) : (240 - index);
+		zeros = padlen;
+	}
+	/*memset(op->buf + j, 0, zeros);*/
+	j += zeros;
+
+	/* TODO use switch */
+	if (op->mode == SS_OP_MD5) {
+		((u32 *)op->buf[cursg])[j / 4] = (op->byte_count << 3) & 0xffffffff;
+		j += 4;
+		((u32 *)op->buf[cursg])[j / 4] = (op->byte_count >> 29) & 0xffffffff;
+		j += 4;
+	} else {
+		if (op->mode == SS_OP_SHA1 || op->mode == SS_OP_SHA224 ||
+		    op->mode == SS_OP_SHA256) {
+			bits = cpu_to_be64(op->byte_count << 3);
+			((u32 *)op->buf[cursg])[j / 4] = bits & 0xffffffff;
+			j += 4;
+			((u32 *)op->buf[cursg])[j / 4] = (bits >> 32) & 0xffffffff;
+			j += 4;
+		} else {
+			bits = cpu_to_be64(op->byte_count >> 61 | op->byte_count2 << 3);
+			((u32 *)op->buf[cursg])[j / 4] = bits & 0xffffffff;
+			j += 4;
+			((u32 *)op->buf[cursg])[j / 4] = (bits >> 32) & 0xffffffff;
+			j += 4;
+			bits = cpu_to_be64(op->byte_count << 3);
+			((u32 *)op->buf[cursg])[j / 4] = bits & 0xffffffff;
+			j += 4;
+			((u32 *)op->buf[cursg])[j / 4] = (bits >> 32) & 0xffffffff;
+			j += 4;
+		}
+	}
+
+	/*dev_info(ss->dev, "Plop j=%d rawdata=%d\n", j, rawdata);*/
+	sun8i_ss_do_task(areq, j + rawdata);
+
+	if ((op->flags & SS_HASH_FINAL) == 0) {
+		op->byte_count += j;
+		memset(op->buf[0], 0, PAGE_SIZE);
+		/*sg_init_table(op->sgbounce, 8);*/
+		for (i = 0; i < 8; i++) {
+			if (op->buf[i]) {
+				memset(op->sgbounce[i], 0,
+				       sizeof(struct scatterlist));
+				sg_set_buf(op->sgbounce[i], op->buf[i],
+					   PAGE_SIZE);
+			}
+		}
+		op->blen = 0;
+	} else {
+		memcpy(areq->result, op->hash, digestsize);
+		/* clean allocated */
+		/*dev_info(ss->dev, "Clean %p\n", op->hash);*/
+		kfree(op->hash);
+		op->hash = NULL;
+		for (i = 0; i < 8; i++) {
+			kfree(op->buf[i]);
+			op->buf[i] = NULL;
+		}
+	}
+
+	return err;
+}
+
+int sun8i_hash_final(struct ahash_request *areq)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sun8i_tfm_ctx *tfmctx = crypto_ahash_ctx(tfm);
+	struct sun8i_ss_ctx *ss = tfmctx->ss;
+	int e = get_engine_number(ss);
+
+	op->flow = e;
+	op->flags = SS_HASH_FINAL;
+	return crypto_transfer_hash_request_to_engine(ss->engines[e], areq);
+}
+
+int sun8i_hash_update(struct ahash_request *areq)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sun8i_tfm_ctx *tfmctx = crypto_ahash_ctx(tfm);
+	struct sun8i_ss_ctx *ss = tfmctx->ss;
+	int e = get_engine_number(ss);
+
+	op->flow = e;
+	op->flags = SS_HASH_UPDATE;
+	return crypto_transfer_hash_request_to_engine(ss->engines[e], areq);
+	return sun8i_hash(areq);
+}
+
+/* sun8i_hash_finup: finalize hashing operation after an update */
+int sun8i_hash_finup(struct ahash_request *areq)
+{
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sun8i_tfm_ctx *tfmctx = crypto_ahash_ctx(tfm);
+	struct sun8i_ss_ctx *ss = tfmctx->ss;
+	int e = get_engine_number(ss);
+
+	op->flow = e;
+	op->flags = SS_HASH_UPDATE | SS_HASH_FINAL;
+	return crypto_transfer_hash_request_to_engine(ss->engines[e], areq);
+	return sun8i_hash(areq);
+}
+
+/* combo of init/update/final functions */
+int sun8i_hash_digest(struct ahash_request *areq)
+{
+	int err;
+	struct sun8i_hash_req_ctx *op = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sun8i_tfm_ctx *tfmctx = crypto_ahash_ctx(tfm);
+	struct sun8i_ss_ctx *ss = tfmctx->ss;
+	int e = get_engine_number(ss);
+
+	err = sun8i_hash_init(areq);
+	if (err != 0)
+		return err;
+
+	op->flow = e;
+	op->flags = SS_HASH_UPDATE | SS_HASH_FINAL;
+	return crypto_transfer_hash_request_to_engine(ss->engines[e], areq);
+	return sun8i_hash(areq);
+}
diff --git a/drivers/staging/sun8i-ss/sun8i-ss.h b/drivers/staging/sun8i-ss/sun8i-ss.h
new file mode 100644
index 0000000..a43a3e5
--- /dev/null
+++ b/drivers/staging/sun8i-ss/sun8i-ss.h
@@ -0,0 +1,176 @@
+#include <linux/crypto.h>
+#include <crypto/hash.h>
+#include <crypto/aes.h>
+#include <crypto/md5.h>
+#include <crypto/engine.h>
+#include <linux/scatterlist.h>
+
+#define CE_TDQ	0x00
+#define CE_CTR	0x04
+#define CE_ICR	0x08
+#define CE_ISR	0x0C
+#define CE_TLR	0x10
+#define CE_TSR	0x14
+#define CE_ESR	0x18
+#define CE_CSSGR	0x1C
+#define CE_CDSGR	0x20
+#define CE_CSAR	0x24
+#define CE_CDAR	0x28
+#define CE_TPR	0x2C
+
+/* Operation direction - bit 8 */
+#define SS_ENCRYPTION		0
+#define SS_DECRYPTION		BIT(8)
+
+/* SS Method - bits 4-6 */
+#define SS_OP_AES		0
+#define SS_OP_DES		1
+#define SS_OP_3DES		2
+#define SS_OP_MD5		16
+#define SS_OP_SHA1		17
+#define SS_OP_SHA224		18
+#define SS_OP_SHA256		19
+#define SS_OP_SHA384		20
+#define SS_OP_SHA512		21
+#define SS_OP_PRNG		49
+
+#define SS_AES_128BITS 0
+#define SS_AES_192BITS 1
+#define SS_AES_256BITS 2
+
+#define SS_CBC	1
+
+#define MAXCHAN 4
+
+struct plop {
+	u32 addr;
+	u32 len;
+} __packed;
+
+struct ss_task {
+	u32 t_id;
+	u32 t_common_ctl;
+	u32 t_sym_ctl;
+	u32 t_asym_ctl;
+	u32 t_key;
+	u32 t_iv;
+	u32 t_ctr;
+	u32 t_dlen;
+	struct plop t_src[8];
+	struct plop t_dst[8];
+	u32 next;
+	u32 reserved[3];
+} __packed __aligned(8);
+
+struct sun8i_ce_chan {
+	struct scatterlist *bounce_src;
+	struct scatterlist *bounce_dst;
+	void *bufsrc;
+	void *bufdst;
+	void *bounce_iv;
+	void *next_iv;
+	struct completion complete;
+	int status;
+};
+
+struct sun8i_ss_ctx {
+	void __iomem *base;
+	void __iomem *nsbase;
+	int irq;
+	int ns_irq;
+	struct clk *busclk;
+	struct clk *ssclk;
+	struct reset_control *reset;
+	struct device *dev;
+	struct resource *res;
+	spinlock_t slock; /* control the use of the device */
+	struct ss_task *tl[8] ____cacheline_aligned;
+	dma_addr_t ce_t_phy[8] ____cacheline_aligned;
+	struct completion complete;
+	int l_flow_complete[8];
+	struct mutex mutex;
+	struct sun8i_ce_chan chanlist[MAXCHAN];
+	struct crypto_engine *engines[MAXCHAN];
+	int flow; /* flow to use in next request */
+};
+
+struct sun8i_cipher_req_ctx {
+	u32 common;
+	u32 sym;
+	int flow;
+};
+
+struct sun8i_hash_req_ctx {
+	struct scatterlist *sgbounce[8];
+	u32 mode;
+	u64 byte_count;
+	u64 byte_count2;/* for sha384 sha512*/
+	u32 *hash;
+	char *buf[8];
+	int sgflag[8];
+	unsigned long blen;
+	unsigned int bsize;
+	int flags;
+	u32 *tiv;
+	int cursg;
+	struct scatterlist *src_sg;
+	/*struct crypto_shash *fallback_tfm;*/
+	int flow;
+};
+
+struct sun8i_tfm_ctx {
+	u32 *key;
+	u32 keylen;
+	u32 keymode;
+	struct sun8i_ss_ctx *ss;
+	struct crypto_blkcipher *fallback_tfm;
+};
+
+struct sun8i_ss_alg_template {
+	u32 type;
+	u32 mode;
+	const void *hash_init;
+	union {
+		struct crypto_alg crypto;
+		struct ahash_alg hash;
+	} alg;
+	struct sun8i_ss_ctx *ss;
+};
+
+int sun8i_ss_cipher(struct ablkcipher_request *areq);
+int sun8i_ss_thread(void *data);
+int sun8i_ce_enqueue(struct crypto_async_request *areq, u32 type);
+
+int sun8i_hash_init(struct ahash_request *areq);
+int sun8i_hash_export_md5(struct ahash_request *areq, void *out);
+int sun8i_hash_import_md5(struct ahash_request *areq, const void *in);
+int sun8i_hash_export_sha1(struct ahash_request *areq, void *out);
+int sun8i_hash_import_sha1(struct ahash_request *areq, const void *in);
+int sun8i_hash_export_sha224(struct ahash_request *areq, void *out);
+int sun8i_hash_import_sha224(struct ahash_request *areq, const void *in);
+int sun8i_hash_export_sha256(struct ahash_request *areq, void *out);
+int sun8i_hash_import_sha256(struct ahash_request *areq, const void *in);
+int sun8i_hash_export_sha512(struct ahash_request *areq, void *out);
+int sun8i_hash_import_sha512(struct ahash_request *areq, const void *in);
+int sun8i_hash_update(struct ahash_request *areq);
+int sun8i_hash_finup(struct ahash_request *areq);
+int sun8i_hash_digest(struct ahash_request *areq);
+int sun8i_hash_final(struct ahash_request *areq);
+int sun8i_hash_crainit(struct crypto_tfm *tfm);
+int sun8i_hash_craexit(struct crypto_tfm *tfm);
+int sun8i_hash(struct ahash_request *areq);
+
+int sun8i_ss_compact(struct scatterlist *sg, unsigned int len);
+int sun8i_ss_bounce_dst(struct ablkcipher_request *areq, int flow);
+int sun8i_ss_bounce_src(struct ablkcipher_request *areq, int flow);
+
+int sun8i_ss_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+		unsigned int keylen);
+int sun8i_ss_cipher_init(struct crypto_tfm *tfm);
+void sun8i_ss_cipher_exit(struct crypto_tfm *tfm);
+int sun8i_ss_cbc_aes_decrypt(struct ablkcipher_request *areq);
+int sun8i_ss_cbc_aes_encrypt(struct ablkcipher_request *areq);
+int handle_cipher_request(struct crypto_engine *engine,
+			  struct ablkcipher_request *breq);
+
+int get_engine_number(struct sun8i_ss_ctx *ss);
